{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07467136",
   "metadata": {},
   "source": [
    "### RAG pipeline: Data ingestion to vector db pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bf5f11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/durlav-bose/Study/Genesys/practice/durlav/rag/rag-practice/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader, PyMuPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258491c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Read all the pdf's inside the directory\n",
    "def process_all_pdfs(pdf_directory):\n",
    "    \"\"\"Process all PDF files in a directory\"\"\"\n",
    "    all_documents = []\n",
    "    pdf_dir = Path(pdf_directory)\n",
    "    \n",
    "    # Find all PDF files recursively\n",
    "    pdf_files = list(pdf_dir.glob(\"**/*.pdf\"))\n",
    "    \n",
    "    print(f\"Found {len(pdf_files)} PDF files to process\")\n",
    "    \n",
    "    for pdf_file in pdf_files:\n",
    "        print(f\"\\nProcessing: {pdf_file.name}\")\n",
    "        try:\n",
    "            loader = PyPDFLoader(str(pdf_file))\n",
    "            documents = loader.load()\n",
    "            \n",
    "            # Add source information to metadata\n",
    "            for doc in documents:\n",
    "                doc.metadata['source_file'] = pdf_file.name\n",
    "                doc.metadata['file_type'] = 'pdf'\n",
    "            \n",
    "            all_documents.extend(documents)\n",
    "            print(f\"  ✓ Loaded {len(documents)} pages\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Error: {e}\")\n",
    "    \n",
    "    print(f\"\\nTotal documents loaded: {len(all_documents)}\")\n",
    "    return all_documents\n",
    "\n",
    "# Process all PDFs in the data directory\n",
    "all_pdf_documents = process_all_pdfs(\"../data/pdf/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f2bc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pdf_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b49ed08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_documents(documents,chunk_size=1000,chunk_overlap=200):\n",
    "    \"\"\"Split documents into smaller chunks for better RAG performance\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "    split_docs = text_splitter.split_documents(documents)\n",
    "    print(f\"Split {len(documents)} documents into {len(split_docs)} chunks\")\n",
    "    \n",
    "    # Show example of a chunk\n",
    "    if split_docs:\n",
    "        print(f\"\\nExample chunk:\")\n",
    "        print(f\"Content: {split_docs[0].page_content[:200]}...\")\n",
    "        print(f\"Metadata: {split_docs[0].metadata}\")\n",
    "    \n",
    "    return split_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcb2986",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks=split_documents(all_pdf_documents)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88070a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "for i, chunk in enumerate(chunks):\n",
    "    counts = Counter(chunk.metadata.get(\"source_file\", \"unknown source\") for chunk in chunks)\n",
    "chunkCount = dict(counts)\n",
    "\n",
    "for source, count in chunkCount.items():\n",
    "    print(f\"Source: {source}, Chunk Count: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531bfd26",
   "metadata": {},
   "source": [
    "### Embedding and vector store db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adaccb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import uuid\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436dc98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingManager:\n",
    "  def __init__(self, model_name: str = \"all-miniLM-L6-v2\"):\n",
    "    self.model_name = model_name\n",
    "    self.model = None\n",
    "    self._load_model()\n",
    "\n",
    "  def _load_model(self):\n",
    "    try:\n",
    "      print(f\"Loading embedding model: {self.model_name}\")\n",
    "      self.model = SentenceTransformer(self.model_name)\n",
    "      embedding_dimension = self.get_embedding_dimension()\n",
    "      print(f\"model loaded successfully. embedding dimension: {embedding_dimension}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "      print(f\"Error loading model: {self.model_name}: {e}\")\n",
    "      raise\n",
    "\n",
    "  def generate_embeddings(self, texts: List[str]) -> np.ndarray:\n",
    "    if not self.model:\n",
    "      raise ValueError(\"Model not loaded\")\n",
    "\n",
    "    print(f\"Generating embedding for {len(texts)} texts\")\n",
    "    embeddings = self.model.encode(texts, show_progress_bar= True)\n",
    "    print(f\"Embedding completed shape {embeddings.shape}\")\n",
    "    return embeddings\n",
    "\n",
    "  def get_embedding_dimension(self) -> int:\n",
    "    if not self.model:\n",
    "      raise ValueError(\"Model not loaded\")\n",
    "    \n",
    "    return self.model.get_sentence_embedding_dimension()\n",
    "\n",
    "    \n",
    "\n",
    "embedding_manager = EmbeddingManager()\n",
    "embedding_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99d9e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorStore:\n",
    "  def __init__(self, collection_name: str = \"pdf_document\", persist_directory: str = \"../data/vector_store\"):\n",
    "    self.collection_name = collection_name\n",
    "    self.persist_directory = persist_directory\n",
    "    self.client = None\n",
    "    self.collection = None\n",
    "    self._initialize_store()\n",
    "\n",
    "  def _initialize_store(self):\n",
    "    try:\n",
    "      #Create persistant chromadb client\n",
    "      os.makedirs(self.persist_directory, exist_ok = True)\n",
    "      self.client = chromadb.PersistentClient(path=self.persist_directory)\n",
    "\n",
    "      #Create or Get collection\n",
    "      self.collection = self.client.get_or_create_collection(\n",
    "        name = self.collection_name, \n",
    "        metadata={\"description\": \"PDF document embedding for RAG\"}\n",
    "      )\n",
    "\n",
    "      print(f\"vector store initialized, collection name {self.collection_name}\")\n",
    "      print(f\"number of collections {self.collection.count()}\")\n",
    "\n",
    "    except Exception as e:\n",
    "      print(f\"Error Initializing vector store {e}\")\n",
    "      raise\n",
    "\n",
    "  def add_documents(self, documents: List[Any], embeddings: np.ndarray):\n",
    "    \"\"\"\n",
    "    Add documents and their embeddings to the vector store\n",
    "    \n",
    "    Args:\n",
    "        documents: List of LangChain documents\n",
    "        embeddings: Corresponding embeddings for the documents\n",
    "    \"\"\"\n",
    "    if len(documents) != len(embeddings):\n",
    "        raise ValueError(\"Number of documents must match number of embeddings\")\n",
    "    \n",
    "    print(f\"Adding {len(documents)} documents to vector store...\")\n",
    "    \n",
    "    # Prepare data for ChromaDB\n",
    "    ids = []\n",
    "    metadatas = []\n",
    "    documents_text = []\n",
    "    embeddings_list = []\n",
    "        \n",
    "    for i, (doc, embedding) in enumerate(zip(documents, embeddings)):\n",
    "      # Generate unique ID\n",
    "      doc_id = f\"doc_{uuid.uuid4().hex[:8]}_{i}\"\n",
    "      ids.append(doc_id)\n",
    "      \n",
    "      # Prepare metadata\n",
    "      metadata = dict(doc.metadata)\n",
    "      metadata['doc_index'] = i\n",
    "      metadata['content_length'] = len(doc.page_content)\n",
    "      metadatas.append(metadata)\n",
    "      \n",
    "      # Document content\n",
    "      documents_text.append(doc.page_content)\n",
    "      \n",
    "      # Embedding\n",
    "      embeddings_list.append(embedding.tolist())\n",
    "    \n",
    "    # Add to collection\n",
    "    try:\n",
    "      self.collection.add(\n",
    "          ids=ids,\n",
    "          embeddings=embeddings_list,\n",
    "          metadatas=metadatas,\n",
    "          documents=documents_text\n",
    "      )\n",
    "      print(f\"Successfully added {len(documents)} documents to vector store\")\n",
    "      print(f\"Total documents in collection: {self.collection.count()}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "      print(f\"Error adding documents to vector store: {e}\")\n",
    "      raise\n",
    "\n",
    "vectorstore=VectorStore()\n",
    "vectorstore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7082779",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [doc.page_content for doc in chunks]\n",
    "embeddings = embedding_manager.generate_embeddings(texts)\n",
    "vector_store = vectorstore.add_documents(chunks, embeddings)\n",
    "vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67b967a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGRetriever:\n",
    "    \"\"\"Handles query-based retrieval from the vector store\"\"\"\n",
    "    \n",
    "    def __init__(self, vector_store: VectorStore, embedding_manager: EmbeddingManager):\n",
    "        \"\"\"\n",
    "        Initialize the retriever\n",
    "        \n",
    "        Args:\n",
    "            vector_store: Vector store containing document embeddings\n",
    "            embedding_manager: Manager for generating query embeddings\n",
    "        \"\"\"\n",
    "        self.vector_store = vector_store\n",
    "        self.embedding_manager = embedding_manager\n",
    "\n",
    "    def retrieve(self, query: str, top_k: int = 10, score_threshold: float = 0.0) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Retrieve relevant documents for a query\n",
    "        \n",
    "        Args:\n",
    "            query: The search query\n",
    "            top_k: Number of top results to return\n",
    "            score_threshold: Minimum similarity score threshold\n",
    "            \n",
    "        Returns:\n",
    "            List of dictionaries containing retrieved documents and metadata\n",
    "        \"\"\"\n",
    "        print(f\"Retrieving documents for query: '{query}'\")\n",
    "        print(f\"Top K: {top_k}, Score threshold: {score_threshold}\")\n",
    "        \n",
    "        # Generate query embedding\n",
    "        query_embedding = self.embedding_manager.generate_embeddings([query])[0]\n",
    "        \n",
    "        # Search in vector store\n",
    "        try:\n",
    "            results = self.vector_store.collection.query(\n",
    "                query_embeddings=[query_embedding.tolist()],\n",
    "                n_results=top_k\n",
    "            )\n",
    "            \n",
    "            # Process results\n",
    "            retrieved_docs = []\n",
    "            \n",
    "            if results['documents'] and results['documents'][0]:\n",
    "                documents = results['documents'][0]\n",
    "                metadatas = results['metadatas'][0]\n",
    "                distances = results['distances'][0]\n",
    "                ids = results['ids'][0]\n",
    "                \n",
    "                for i, (doc_id, document, metadata, distance) in enumerate(zip(ids, documents, metadatas, distances)):\n",
    "                    # Convert distance to similarity score (ChromaDB uses cosine distance)\n",
    "                    similarity_score = 1 - distance\n",
    "                    \n",
    "                    if similarity_score >= score_threshold:\n",
    "                        retrieved_docs.append({\n",
    "                            'id': doc_id,\n",
    "                            'content': document,\n",
    "                            'metadata': metadata,\n",
    "                            'similarity_score': similarity_score,\n",
    "                            'distance': distance,\n",
    "                            'rank': i + 1\n",
    "                        })\n",
    "                \n",
    "                print(f\"Retrieved {len(retrieved_docs)} documents (after filtering)\")\n",
    "            else:\n",
    "                print(\"No documents found\")\n",
    "            \n",
    "            return retrieved_docs\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error during retrieval: {e}\")\n",
    "            return []\n",
    "\n",
    "rag_retriever=RAGRetriever(vectorstore,embedding_manager)\n",
    "rag_retriever\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54017e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. First, check if the sentence exists in your chunks\n",
    "search_text = \"who is the author of the paper A Review Paper about Deep Learning for Medical Image Analysis\"\n",
    "matching_chunks = [\n",
    "    (i, chunk.page_content) \n",
    "    for i, chunk in enumerate(chunks) \n",
    "    if search_text.lower() in chunk.page_content.lower()\n",
    "]\n",
    "\n",
    "print(f\"Found {len(matching_chunks)} chunks containing the text:\")\n",
    "for idx, content in matching_chunks:\n",
    "    print(f\"\\nChunk {idx}:\\n{content}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45913368",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs = rag_retriever.retrieve(\"who is the author of the paper A Review Paper about Deep Learning for Medical Image Analysis\", top_k=5, score_threshold=0.1)\n",
    "print(f\"Retrieved {retrieved_docs[0]} documents from RAG retriever:\")\n",
    "informations = []\n",
    "print(f\"length of retrieved docs {len(retrieved_docs)}\")\n",
    "for doc in retrieved_docs:\n",
    "    print(f\"metadata: {doc['metadata']}\")\n",
    "    content = doc['content']\n",
    "    author = doc['metadata']['author'] if 'author' in doc['metadata'] else 'Unknown'\n",
    "    page_number = doc['metadata']['page_number'] if 'page_number' in doc['metadata'] else 'Unknown'\n",
    "    title = doc['metadata']['title'] if 'title' in doc['metadata'] else 'Unknown'\n",
    "    informations.append(f\"title: {title}, author: {author}, page number: {page_number}, content: {content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7bee5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "class ollamaRAG:\n",
    "    def __init__(self, model_name: str = \"llama3.2:3b\", temperature: float = 0.3):\n",
    "        self.model_name = model_name\n",
    "        self.temperature = temperature\n",
    "        self.llm = ChatOllama(model=self.model_name, temperature=self.temperature)\n",
    "        self.prompt_template = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"\"\"You are a document assistant. You must STRICTLY follow these rules:\n",
    "1. Answer ONLY using information from the provided context\n",
    "2. Do NOT use your general knowledge or training data\n",
    "3. If the answer is not in the context, respond EXACTLY with: \"I don't know - this information is not in the provided documents.\"\n",
    "4. Do NOT make assumptions or infer information not explicitly stated in the context\n",
    "5. Quote relevant parts from the context when answering\"\"\"),\n",
    "            (\"user\", \"\"\"Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer based ONLY on the context above:\"\"\")\n",
    "        ])\n",
    "\n",
    "    def ask(self, question: str, retrieved_chunks: List[str]) -> str:\n",
    "        context = \"\\n\\n---\\n\\n\".join(retrieved_chunks)\n",
    "        print(f\"Constructed context for question:\\n{context[:500]}...\")  # Show a preview of the context\n",
    "\n",
    "        res = self.prompt_template | self.llm\n",
    "        response = res.invoke({\n",
    "            \"context\": context,\n",
    "            \"question\": question\n",
    "        })\n",
    "\n",
    "        return response.content\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "rag = ollamaRAG()\n",
    "answer = rag.ask(\"who is the author of the paper A Review Paper about Deep Learning for Medical Image Analysis?\", [doc for doc in informations])\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b263a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28913cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-practice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
